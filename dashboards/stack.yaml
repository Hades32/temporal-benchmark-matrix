title: Stack

editable: true
shared_crosshair: true
auto_refresh: 30s
time: [now-1h, now]

rows:
  - name: Metering
    panels:
      - timeseries:
          title: State Transitions
          datasource: prometheus
          targets:
            - prometheus:
                query: sum(rate(state_transition_count_count[$__rate_interval]))
                legend: "st/s"
  - name: Polling
    panels:
      - timeseries:
          title: Polling Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(le) (rate(temporal_long_request_latency_bucket{job="benchmark-monitoring",namespace="default",operation="PollWorkflowTaskQueue"}[$__rate_interval])))
                legend: Workflow
            - prometheus:
                query: histogram_quantile(0.95, sum by(le) (rate(temporal_long_request_latency_bucket{job="benchmark-monitoring",namespace="default",operation="PollActivityTaskQueue"}[$__rate_interval])))
                legend: Activity
          axis:
            unit: s
      - timeseries:
          title: Poll Timeouts
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (task_type) (rate(poll_timeouts{namespace="default"}[$__rate_interval]))
                legend: "{{task_type}}"
      - timeseries:
          title: Poller Count
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (poller_type) (temporal_num_pollers{namespace="default"})
                legend: "{{poller_type}}"
  - name: Matching
    panels:
      - timeseries:
          title: Poll Sync
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (task_type) (rate(poll_success_sync[$__rate_interval])) / sum by (task_type) (rate(poll_success[$__rate_interval]))
                legend: "{{task_type}}"
          axis:
            unit: percentunit
      - timeseries:
          title: Asyncmatch Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(asyncmatch_latency_bucket{service_name=~"matching"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Matching Errors
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by(error_type) (rate(service_error_with_type{service_name="matching"}[$__rate_interval]))
                legend: "{{error_type}}"
  - name: Soak Tester
    panels:
      - timeseries:
          title: Requests
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (operation) (rate(temporal_request{container="benchmark-soak-test"}[$__rate_interval]))
                legend: "{{operation}}"
      - timeseries:
          title: Request Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(temporal_request_latency_bucket{container="benchmark-soak-test"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Long Poll Requests
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (operation) (rate(temporal_long_request{container="benchmark-soak-test"}[$__rate_interval]))
                legend: "{{operation}}"
      - timeseries:
          title: Long Poll Request Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(temporal_long_request_latency_bucket{container="benchmark-soak-test"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
  - name: Schedule to Start
    panels:
      - timeseries:
          title: Workflow Task Schedule To Start Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(le) (rate(temporal_workflow_task_schedule_to_start_latency_bucket{job="benchmark-monitoring",namespace="default"}[$__rate_interval])))
                legend: p95
          axis:
            unit: s
      - timeseries:
          title: Activity Schedule To Start Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(le) (rate(temporal_activity_schedule_to_start_latency_bucket{job="benchmark-monitoring",namespace="default"}[$__rate_interval])))
                legend: p95
          axis:
            unit: s
  - name: Workers
    panels:
      - timeseries:
          title: Requests
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (operation) (rate(temporal_request{container="benchmark-workers"}[$__rate_interval]))
                legend: "{{operation}}"
      - timeseries:
          title: Request Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(temporal_request_latency_bucket{container="benchmark-workers"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Slots Available
          datasource: prometheus
          targets:
            - prometheus:
                query: avg by (worker_type) (temporal_worker_task_slots_available{container="benchmark-workers"})
                legend: "{{$worker_type}}"
      - timeseries:
          title: Sticky Cache
          datasource: prometheus
          targets:
            - prometheus:
                query: sum(rate(temporal_sticky_cache_hit{container="benchmark-workers"}[$__rate_interval]))
                legend: "hit"
            - prometheus:
                query: sum(rate(temporal_sticky_cache_miss{container="benchmark-workers"}[$__rate_interval]))
                legend: "miss"
            - prometheus:
                query: sum(rate(temporal_sticky_cache_total_forced_eviction{container="benchmark-workers"}[$__rate_interval]))
                legend: "eviction"
  - name: History
    panels:
      - timeseries:
          title: Task requests
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (operation) (rate(task_requests[$__rate_interval]))
                legend: "{{operation}}"
      - timeseries:
          title: Task errors
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (operation) (rate(task_errors[$__rate_interval]))
                legend: "{{operation}}"
      - timeseries:
          title: Task processing latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(task_latency_processing_bucket[1m])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Task latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(task_latency_bucket[1m])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Task queue latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(task_latency_queue_bucket[1m])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Task load latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(task_latency_load_bucket[1m])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Task schedule latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(task_latency_schedule_bucket[1m])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Task user latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by (operation, le) (rate(task_latency_user_bucket[1m])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: History Errors
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by(error_type) (rate(service_error_with_type{service_name="history"}[$__rate_interval]))
                legend: "{{error_type}}"
      - timeseries:
          title: Shard distribution
          datasource: prometheus
          targets:
            - prometheus:
                query: numshards_gauge
                legend: "{{pod}}"
  - name: Persistence
    panels:
      - timeseries:
          title: Requests
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (operation) (rate(persistence_requests[1m]))
                legend: "{{operation}}"
      - timeseries:
          title: Workflow Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(operation, le) (rate(persistence_latency_bucket{operation=~".*Workflow.*"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Task Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(operation, le) (rate(persistence_latency_bucket{operation=~".*Task.*"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Shard Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(operation, le) (rate(persistence_latency_bucket{operation=~".*Shard.*"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Misc Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(operation, le) (rate(persistence_latency_bucket{operation=~".*(Cluster|Namespace).*"}[$__rate_interval])))
                legend: "{{operation}}"
          axis:
            unit: s
      - timeseries:
          title: Errors
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (error_type) (rate(persistence_error_with_type[$__rate_interval]))
                legend: "{{error_type}}"
  - name: Locking
    panels:
      - timeseries:
          title: Workflow Lock Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(le) (rate(cache_latency_bucket{operation="HistoryCacheGetOrCreate"}[$__rate_interval])))
                legend: p95
          axis:
            unit: s
      - timeseries:
          title: Shard Lock Latency
          datasource: prometheus
          targets:
            - prometheus:
                query: histogram_quantile(0.95, sum by(le) (rate(lock_latency_bucket{operation="ShardInfo"}[$__rate_interval])))
                legend: p95
          axis:
            unit: s
  - name: Resource Exhausted
    panels:
      - timeseries:
          title: Errors
          datasource: prometheus
          targets:
            - prometheus:
                query: sum by (resource_exhausted_cause, operation) (rate(service_errors_resource_exhausted[$__rate_interval]))
                legend: "{{resource_exhausted_cause}}: {{operation}}"
            - prometheus:
                query: sum (rate(persistence_errors_resource_exhausted[$__rate_interval]))
                legend: Persistence